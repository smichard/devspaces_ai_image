schemaVersion: 2.2.0
metadata:
  name: ollama-workspace
  label: ollama
attributes:
  controller.devfile.io/storage-type: ephemeral
components:
- name: udi
  container:
    image: quay.io/michard/devspaces_base_image:0.1.17
    #image: quay.io/devfile/universal-developer-image:ubi8-latest
    memoryLimit: 4Gi
    memoryRequest: 2Gi
    cpuLimit: 4000m
    cpuRequest: 1000m
    mountSources: true
    sourceMapping: /projects
- name: ollama
  attributes:
    container-overrides:
      resources:
        limits:
          cpu: 4000m
          memory: 16Gi
          nvidia.com/gpu: 1 # limiting to 1 GPU
        requests:
          cpu: 1000m
          memory: 8Gi
          nvidia.com/gpu: 1 # requesting 1 GPU
  container:
    image: docker.io/ollama/ollama:latest
    mountSources: true
    sourceMapping: /.ollama
commands:
  - id: pullmodel
    exec:
      component: ollama
      commandLine: "ollama pull codellama:13b"
  - id: copyconfig
    exec:
      component: udi
      commandLine: "mkdir /home/user/.continue && cp /projects/devspaces-ai-image/continue-config.json /home/user/.continue/config.json"
events:
  postStart:
    - pullmodel
    - copyconfig
